{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'FlattenObservation' object has no attribute 'define_spaces'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 25\u001b[0m\n\u001b[1;32m     23\u001b[0m env \u001b[38;5;241m=\u001b[39m ParkingWithObstacles(env_origin)\n\u001b[1;32m     24\u001b[0m env \u001b[38;5;241m=\u001b[39m FlattenObservation(env)\n\u001b[0;32m---> 25\u001b[0m \u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdefine_spaces\u001b[49m()\n\u001b[1;32m     27\u001b[0m \u001b[38;5;66;03m# terminated The episode is over if the ego vehicle crashed or the goal is reached or time is over.\u001b[39;00m\n\u001b[1;32m     28\u001b[0m terminated \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'FlattenObservation' object has no attribute 'define_spaces'"
     ]
    }
   ],
   "source": [
    "# Our custom module\n",
    "from nn import DeepQNetwork\n",
    "from custom_parking_env import ParkingWithObstacles\n",
    "from relay_mem import ReplayMemory,Transition\n",
    "\n",
    "# python module\n",
    "import gymnasium as gym\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.nn as nn\n",
    "from itertools import count\n",
    "from IPython import display\n",
    "from gymnasium.wrappers import FlattenObservation\n",
    "\n",
    "# interactive mode\n",
    "plt.ion()\n",
    "\n",
    "# Create the original environment and wrap it into an environment with obstacles\n",
    "env_origin = gym.make(\"parking-v0\", render_mode=\"human\")\n",
    "env = ParkingWithObstacles(env_origin)\n",
    "env = FlattenObservation(env)\n",
    "env.define_spaces()\n",
    "\n",
    "# terminated The episode is over if the ego vehicle crashed or the goal is reached or time is over.\n",
    "terminated = False\n",
    "\n",
    "# The episode is truncated if the time is over.\n",
    "truncated = False\n",
    "\n",
    "# Print the number of states and actions\n",
    "print(f\"Observation Space: {env.observation_space}\")\n",
    "# observation[\"observation\"] Box(-inf, inf, (6,), float64)\n",
    "# observation[\"achieved_goal\"] Box(-inf, inf, (6,), float64)\n",
    "# observation[\"desired_goal\"] Box(-inf, inf, (6,), float64)\n",
    "\n",
    "print(f\"Action Space: {env.action_space}\")\n",
    "# Discrete(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'Box' object has no attribute 'n'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 24\u001b[0m\n\u001b[1;32m     21\u001b[0m LR \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1e-4\u001b[39m\n\u001b[1;32m     23\u001b[0m \u001b[38;5;66;03m# Get number of actions from gym action space\u001b[39;00m\n\u001b[0;32m---> 24\u001b[0m n_actions \u001b[38;5;241m=\u001b[39m \u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43maction_space\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mn\u001b[49m\n\u001b[1;32m     25\u001b[0m \u001b[38;5;66;03m# Get the number of state observations\u001b[39;00m\n\u001b[1;32m     26\u001b[0m state, info \u001b[38;5;241m=\u001b[39m env\u001b[38;5;241m.\u001b[39mreset()\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'Box' object has no attribute 'n'"
     ]
    }
   ],
   "source": [
    "# if GPU is to be used\n",
    "device = torch.device(\n",
    "    \"cuda\" if torch.cuda.is_available() else\n",
    "    \"mps\" if torch.backends.mps.is_available() else\n",
    "    \"cpu\"\n",
    ")\n",
    "\n",
    "# BATCH_SIZE is the number of transitions sampled from the replay buffer\n",
    "# GAMMA is the discount factor as mentioned in the previous section\n",
    "# EPS_START is the starting value of epsilon\n",
    "# EPS_END is the final value of epsilon\n",
    "# EPS_DECAY controls the rate of exponential decay of epsilon, higher means a slower decay\n",
    "# TAU is the update rate of the target network\n",
    "# LR is the learning rate of the ``AdamW`` optimizer\n",
    "BATCH_SIZE = 128\n",
    "GAMMA = 0.99\n",
    "EPS_START = 0.9\n",
    "EPS_END = 0.05\n",
    "EPS_DECAY = 1000\n",
    "TAU = 0.005\n",
    "LR = 1e-4\n",
    "\n",
    "# Get number of actions from gym action space\n",
    "n_actions = env.action_space.n\n",
    "\n",
    "# Get the number of state observations\n",
    "state, info = env.reset()\n",
    "n_observations = len(state)\n",
    "\n",
    "policy_net = DeepQNetwork(n_observations, n_actions).to(device)\n",
    "target_net = DeepQNetwork(n_observations, n_actions).to(device)\n",
    "target_net.load_state_dict(policy_net.state_dict())\n",
    "\n",
    "optimizer = optim.AdamW(policy_net.parameters(), lr=LR, amsgrad=True)\n",
    "memory = ReplayMemory(10000)\n",
    "\n",
    "steps_done = 0\n",
    "\n",
    "# Training loop\n",
    "num_episodes = 1500\n",
    "epsilon_decay_rate = -np.log(EPS_END / EPS_START) / num_episodes\n",
    "epsilon_values = []\n",
    "episode_rewards = []  # Store total rewards per episode\n",
    "losses = []\n",
    "episode_durations = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_action(state,i_episode):\n",
    "    global steps_done\n",
    "    sample = random.random()\n",
    "    epsilon_threshold = EPS_START * np.exp(-epsilon_decay_rate * i_episode)\n",
    "    epsilon_values.append(epsilon_threshold)\n",
    "    steps_done += 1\n",
    "    if sample > epsilon_threshold:\n",
    "        with torch.no_grad():\n",
    "            # t.max(1) will return the largest column value of each row.\n",
    "            # second column on max result is index of where max element was\n",
    "            # found, so we pick action with the larger expected reward.\n",
    "            return policy_net(state).max(1).indices.view(1, 1)\n",
    "    else:\n",
    "        return torch.tensor([[env.action_space.sample()]], device=device, dtype=torch.long)\n",
    "\n",
    "def optimize_policy_net():\n",
    "    # Check if enough transitions are available in replay_buffer\n",
    "    if len(memory) < BATCH_SIZE:\n",
    "        return\n",
    "\n",
    "    # Sample a batch of transitions\n",
    "    transitions = memory.sample(BATCH_SIZE)\n",
    "    batch = Transition(*zip(*transitions))\n",
    "\n",
    "    # Create a mask for non-final next states\n",
    "    non_final_mask = torch.tensor(tuple(map(lambda s: s is not None, batch.next_state)), device=device, dtype=torch.bool)\n",
    "    non_final_next_states = torch.cat([s for s in batch.next_state if s is not None])\n",
    "    \n",
    "    # Concatenate batch tensors\n",
    "    state_batch = torch.cat(batch.state)\n",
    "    action_batch = torch.cat(batch.action)\n",
    "    reward_batch = torch.cat(batch.reward)\n",
    "\n",
    "    # Compute Q values for the current state-action pairs\n",
    "    state_action_values = policy_net(state_batch).gather(1, action_batch)\n",
    "\n",
    "    # Compute Q values for the next states\n",
    "    next_state_values = torch.zeros(BATCH_SIZE, device=device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        next_state_values[non_final_mask] = target_net(non_final_next_states).max(1).values\n",
    "    \n",
    "    # Compute the expected Q values\n",
    "    expected_state_action_values = (next_state_values * GAMMA) + reward_batch\n",
    "\n",
    "    # Compute Huber loss\n",
    "    criterion = nn.SmoothL1Loss()\n",
    "    loss = criterion(state_action_values, expected_state_action_values.unsqueeze(1))\n",
    "\n",
    "    # Optimize the policy network\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "\n",
    "    # In-place gradient clipping\n",
    "    # for param in policy_net.parameters():\n",
    "    #     param.grad.data.clamp_(-1, 1)\n",
    "    torch.nn.utils.clip_grad_value_(policy_net.parameters(), 100)\n",
    "    optimizer.step()\n",
    "\n",
    "    # Track the loss for plotting\n",
    "    losses.append(loss.item())\n",
    "\n",
    "def moving_average(data, window_size=50):\n",
    "    # convolution\n",
    "    return np.convolve(data, np.ones(window_size)/window_size, mode='valid')\n",
    "\n",
    "def plot_training_durations():\n",
    "    # Create a new figure for the plot\n",
    "    plt.clf()\n",
    "\n",
    "    # Convert episode rewards to tensors\n",
    "    rewards_t = torch.tensor(episode_rewards, dtype=torch.float)\n",
    "\n",
    "    # Plot rewards per episode\n",
    "    plt.subplot(3, 1, 1)\n",
    "    plt.title('Training Progress (Per Episode)')\n",
    "    plt.xlabel('Episode')\n",
    "    plt.ylabel('Total Reward')\n",
    "    plt.plot(rewards_t.numpy(), label=\"Total Reward\")\n",
    "    \n",
    "    # Add smoothed rewards\n",
    "    if len(episode_rewards) >= 50:  # Ensure enough data for smoothing\n",
    "        smoothed_rewards = moving_average(episode_rewards, window_size=50)\n",
    "        plt.plot(range(len(smoothed_rewards)), smoothed_rewards, label=\"Smoothed Rewards\", color='orange')\n",
    "    plt.legend()\n",
    "\n",
    "    # Plot losses per step\n",
    "    plt.subplot(3, 1, 2)\n",
    "    plt.title('Loss (Per Step)')\n",
    "    plt.xlabel('Training Step')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.plot(losses, label=\"Loss\", color='red')\n",
    "    plt.legend()\n",
    "\n",
    "    # Plot epsilon decay per step\n",
    "    plt.subplot(3, 1, 3)\n",
    "    plt.title('Epsilon Decay (Per Step)')\n",
    "    plt.xlabel('Training Step')\n",
    "    plt.ylabel('Epsilon')\n",
    "    plt.plot(epsilon_values, label=\"Epsilon Decay\", color='blue')\n",
    "    plt.legend()\n",
    "\n",
    "    # Adjust layout to increase space between plots\n",
    "    plt.subplots_adjust(hspace=0.5)  # Adjust spacing between rows\n",
    "\n",
    "    # Pause briefly to update the plot\n",
    "    plt.pause(0.01)\n",
    "\n",
    "    # Clear the current output and display the updated plot\n",
    "    display.display(plt.gcf())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i_episode in range(num_episodes):\n",
    "    # Initialize the environment and get its state\n",
    "    state, info = env.reset()\n",
    "    \n",
    "    for t in count():\n",
    "        action = select_action(state,i_episode)\n",
    "        observation, reward, terminated, truncated, _ = env.step(action.item())\n",
    "        reward = torch.tensor([reward], device=device)\n",
    "        done = terminated or truncated\n",
    "\n",
    "        if terminated:\n",
    "            next_state = None\n",
    "        else:\n",
    "            next_state = torch.tensor(observation, dtype=torch.float32, device=device).unsqueeze(0)\n",
    "\n",
    "        # Store the transition in memory\n",
    "        memory.push(state, action, next_state, reward)\n",
    "\n",
    "        # Move to the next state\n",
    "        state = next_state\n",
    "\n",
    "        # Perform one step of the optimization (on the policy network)\n",
    "        optimize_policy_net()\n",
    "\n",
    "        # Soft update of the target network's weights\n",
    "        target_net_state_dict = target_net.state_dict()\n",
    "        policy_net_state_dict = policy_net.state_dict()\n",
    "        for key in policy_net_state_dict:\n",
    "            target_net_state_dict[key] = policy_net_state_dict[key]*TAU + target_net_state_dict[key]*(1-TAU)\n",
    "        target_net.load_state_dict(target_net_state_dict)\n",
    "\n",
    "        if done:\n",
    "            episode_durations.append(t + 1)\n",
    "            plot_training_durations()\n",
    "            break\n",
    "\n",
    "print('Complete')\n",
    "plot_training_durations(show_result=True)\n",
    "plt.ioff()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
