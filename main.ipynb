{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Our custom module\n",
    "from nn import DeepQNetwork\n",
    "from custom_parking_env import ParkingWithObstacles\n",
    "from relay_mem import ReplayMemory,Transition\n",
    "\n",
    "# python module\n",
    "import gymnasium as gym\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.nn as nn\n",
    "from itertools import count\n",
    "from IPython import display\n",
    "\n",
    "# combine observation\n",
    "def process_observation(observation):\n",
    "    # observation_vector = np.concatenate((\n",
    "    #     observation[\"observation\"],\n",
    "    #     observation[\"achieved_goal\"],\n",
    "    #     observation[\"desired_goal\"]\n",
    "    # ))\n",
    "    return observation[\"observation\"]\n",
    "\n",
    "# interactive mode\n",
    "plt.ion()\n",
    "\n",
    "# Create the original environment and wrap it into an environment with obstacles\n",
    "env_origin = gym.make(\"parking-v0\", render_mode=\"human\")\n",
    "env = ParkingWithObstacles(env_origin)\n",
    "env.define_spaces()\n",
    "\n",
    "# terminated The episode is over if the ego vehicle crashed or the goal is reached or time is over.\n",
    "terminated = False\n",
    "\n",
    "# The episode is truncated if the time is over.\n",
    "truncated = False\n",
    "\n",
    "# Print the number of states and actions\n",
    "print(f\"Observation Space: {env.observation_space}\")\n",
    "# observation[\"observation\"] Box(-inf, inf, (6,), float64)\n",
    "# observation[\"achieved_goal\"] Box(-inf, inf, (6,), float64)\n",
    "# observation[\"desired_goal\"] Box(-inf, inf, (6,), float64)\n",
    "\n",
    "print(f\"Action Space: {env.action_space}\")\n",
    "# Discrete(5)\n",
    "\n",
    "\n",
    "# Get the number of state observations\n",
    "state, info = env.reset()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if GPU is to be used\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "    print(\"Using CUDA\")\n",
    "elif torch.backends.mps.is_available():\n",
    "    device = torch.device(\"mps\")\n",
    "    print(\"Using MPS\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    print(\"Using CPU\")\n",
    "    \n",
    "candidate_actions = []\n",
    "\n",
    "for steering in np.linspace(-0.5, 0.5, 3):\n",
    "        for acceleration in np.linspace(0.8, 0.4, 2):\n",
    "            candidate_actions.append(torch.Tensor([acceleration, steering],device=device))\n",
    "            \n",
    "# print(candidate_actions)\n",
    "\n",
    "# Get number of actions from gym action space\n",
    "n_actions = len(candidate_actions)\n",
    "n_observations = len(process_observation(state))  # 6 (observation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# BATCH_SIZE is the number of transitions sampled from the replay buffer\n",
    "# GAMMA is the discount factor as mentioned in the previous section\n",
    "# EPS_START is the starting value of epsilon\n",
    "# EPS_END is the final value of epsilon\n",
    "# EPS_DECAY controls the rate of exponential decay of epsilon, higher means a slower decay\n",
    "# TAU is the update rate of the target network\n",
    "# LR is the learning rate of the ``AdamW`` optimizer\n",
    "BATCH_SIZE = 64\n",
    "GAMMA = 0.99\n",
    "EPS_START = 1\n",
    "EPS_END = 0.05\n",
    "# EPS_DECAY = 1000\n",
    "# TAU = 0.005\n",
    "TARGET_UPDATE = 10\n",
    "LR = 0.0002\n",
    "EPISODES = 500\n",
    "\n",
    "policy_net = DeepQNetwork(n_observations, n_actions).to(device)\n",
    "target_net = DeepQNetwork(n_observations, n_actions).to(device)\n",
    "# init weight (by policy net)\n",
    "target_net.load_state_dict(policy_net.state_dict())\n",
    "\n",
    "optimizer = optim.AdamW(policy_net.parameters(), lr=LR, amsgrad=True)\n",
    "memory = ReplayMemory(10000)\n",
    "\n",
    "# Training loop\n",
    "num_episodes = EPISODES\n",
    "epsilon_decay_rate = -np.log(EPS_END / EPS_START) / num_episodes\n",
    "epsilon_values = []\n",
    "episode_rewards = []  # Store total rewards per episode\n",
    "losses = []\n",
    "episode_durations = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "steps_done = 0\n",
    "def select_action(state , i_episode):\n",
    "    global steps_done\n",
    "    sample = random.random()\n",
    "    epsilon_threshold = EPS_START * np.exp(-epsilon_decay_rate * i_episode)\n",
    "    epsilon_values.append(epsilon_threshold)\n",
    "    steps_done += 1\n",
    "    # exploitation\n",
    "    if sample > epsilon_threshold:\n",
    "        with torch.no_grad():\n",
    "            # t.max(1) will return the largest column value of each row.\n",
    "            # second column on max result is index of where max element was\n",
    "            # found, so we pick action with the larger expected reward.\n",
    "            # tensor 1x1 , return action index\n",
    "            return policy_net(state).max(1).indices.view(1, 1)\n",
    "    # exploration\n",
    "    else:\n",
    "        return torch.tensor([[random.randrange(n_actions)]], device=device, dtype=torch.long)\n",
    "\n",
    "def optimize_policy_net():\n",
    "    # Check if enough transitions are available in replay_buffer\n",
    "    if len(memory) < BATCH_SIZE:\n",
    "        return\n",
    "\n",
    "    # Sample a batch of transitions\n",
    "    transitions = memory.sample(BATCH_SIZE)\n",
    "    batch = Transition(*zip(*transitions))\n",
    "\n",
    "    # Compute a mask of non-final states and concatenate the batch elements\n",
    "    # (a final state would've been the one after which simulation ended)\n",
    "    non_final_mask = torch.tensor(\n",
    "                        tuple(map(lambda s: s is not None,\n",
    "                        batch.next_state)), \n",
    "                        device=device, \n",
    "                        dtype=torch.bool)\n",
    "    \n",
    "    non_final_next_states = []\n",
    "    for s in batch.next_state:\n",
    "        if s is not None:  # Check if s is not None\n",
    "            non_final_next_states.append(s)  # Add elements that meet the condition to the list\n",
    "    \n",
    "    if len(non_final_next_states) == 0 :\n",
    "        # Use a placeholder tensor with the appropriate shape\n",
    "        # Assuming the stqate has a known shape, e.g., (batch_size, state_dim)\n",
    "        non_final_next_states = tuple(torch.zeros(torch.Size([1, 18]), device=device).unsqueeze(0))\n",
    "        print(\"none occur\")\n",
    "\n",
    "    # RuntimeError: torch.cat(): expected a non-empty list of Tensors\n",
    "    non_final_next_states = torch.cat(non_final_next_states)\n",
    "    \n",
    "    # Concatenate batch tensors\n",
    "    state_batch = torch.cat(batch.state)\n",
    "    action_batch = torch.cat(batch.action)\n",
    "    reward_batch = torch.cat(batch.reward)\n",
    "\n",
    "    # Compute Q(s_t, a) - the model computes Q(s_t), then we select the\n",
    "    # columns of actions taken. These are the actions which would've been taken\n",
    "    # for each batch state according to policy_net\n",
    "    state_action_values = policy_net(state_batch).gather(1, action_batch)\n",
    "\n",
    "    # Compute V(s_{t+1}) for all next states.\n",
    "    # Expected values of actions for non_final_next_states are computed based\n",
    "    # on the \"older\" target_net; selecting their best reward with max(1).values\n",
    "    # This is merged based on the mask, such that we'll have either the expected\n",
    "    # state value or 0 in case the state was final.\n",
    "    next_state_values = torch.zeros(BATCH_SIZE, device=device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        next_state_values[non_final_mask] = target_net(non_final_next_states).max(1).values\n",
    "    \n",
    "    # Compute the expected Q values\n",
    "    expected_state_action_values = (next_state_values * GAMMA) + reward_batch\n",
    "\n",
    "    # Compute Huber loss\n",
    "    criterion = nn.SmoothL1Loss()\n",
    "    loss = criterion(state_action_values, expected_state_action_values.unsqueeze(1))\n",
    "\n",
    "    # Optimize the policy network\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "\n",
    "    # In-place gradient clipping\n",
    "    # for param in policy_net.parameters():\n",
    "    #     param.grad.data.clamp_(-1, 1)\n",
    "    torch.nn.utils.clip_grad_value_(policy_net.parameters(), 1)\n",
    "    optimizer.step()\n",
    "\n",
    "    # Track the loss for plotting\n",
    "    losses.append(loss.item())\n",
    "    print(f\"Loss: {loss.item()}\")\n",
    "\n",
    "def moving_average(data, window_size=50):\n",
    "    # convolution\n",
    "    return np.convolve(data, np.ones(window_size)/window_size, mode='valid')\n",
    "\n",
    "def plot_training_durations():\n",
    "    # Create a new figure for the plot\n",
    "    plt.clf()\n",
    "\n",
    "    # Convert episode rewards to tensors\n",
    "    rewards_t = torch.tensor(episode_rewards, dtype=torch.float, device=device)\n",
    "\n",
    "    # Plot rewards per episode\n",
    "    plt.subplot(3, 1, 1)\n",
    "    plt.title('Training Progress (Per Episode)')\n",
    "    plt.xlabel('Episode')\n",
    "    plt.ylabel('Total Reward')\n",
    "    \n",
    "    # Move GPU tensor back to CPU and convert it to NumPy\n",
    "    plt.plot(rewards_t.cpu().numpy(), label=\"Total Reward\")\n",
    "    \n",
    "    # Add smoothed rewards\n",
    "    if len(episode_rewards) >= 50:  # Ensure enough data for smoothing\n",
    "        smoothed_rewards = moving_average(episode_rewards, window_size=50)\n",
    "        plt.plot(range(len(smoothed_rewards)), smoothed_rewards, label=\"Smoothed Rewards\", color='orange')\n",
    "    plt.legend()\n",
    "\n",
    "    # Plot losses per step\n",
    "    plt.subplot(3, 1, 2)\n",
    "    plt.title('Loss (Per Step)')\n",
    "    plt.xlabel('Training Step')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.plot(losses, label=\"Loss\", color='red')\n",
    "    plt.legend()\n",
    "\n",
    "    # Plot epsilon decay per step\n",
    "    plt.subplot(3, 1, 3)\n",
    "    plt.title('Epsilon Decay (Per Step)')\n",
    "    plt.xlabel('Training Step')\n",
    "    plt.ylabel('Epsilon')\n",
    "    plt.plot(epsilon_values, label=\"Epsilon Decay\", color='blue')\n",
    "    plt.legend()\n",
    "\n",
    "    # Adjust layout to increase space between plots\n",
    "    plt.subplots_adjust(hspace=0.5)  # Adjust spacing between rows\n",
    "\n",
    "    # Pause briefly to update the plot\n",
    "    plt.pause(0.01)\n",
    "\n",
    "    # Clear the current output and display the updated plot\n",
    "    display.display(plt.gcf())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i_episode in range(num_episodes):\n",
    "    # Initialize the environment and get its state\n",
    "    observation, info = env.reset()\n",
    "    observation_vector = process_observation(observation)\n",
    "    # 1x6 tensor\n",
    "    observation_tensor = torch.tensor(observation_vector, dtype=torch.float32, device=device).unsqueeze(0)\n",
    "    total_reward = 0  # Track total reward for this episode\n",
    "\n",
    "    for t in range(100):\n",
    "        action_index = select_action(observation_tensor,i_episode)\n",
    "        action = candidate_actions[action_index]\n",
    "        observation, reward, done, truncated, _ = env.step(action.numpy())\n",
    "        reward = torch.tensor([reward], device=device)\n",
    "\n",
    "        total_reward += reward.item()  # Accumulate rewards\n",
    "\n",
    "        # done = terminated or truncated\n",
    "\n",
    "        # if truncated:\n",
    "        #     next_observation_tensor = None\n",
    "        # else:\n",
    "        #     observation_vector = process_observation(observation)\n",
    "        #     next_observation_tensor = torch.tensor(observation_vector, dtype=torch.float32, device=device).unsqueeze(0)\n",
    "\n",
    "        observation_vector = process_observation(observation)\n",
    "        next_observation_tensor = torch.tensor(observation_vector, dtype=torch.float32, device=device).unsqueeze(0)\n",
    "\n",
    "        # Store the transition in memory\n",
    "        memory.push(observation_tensor, action_index, next_observation_tensor, reward)\n",
    "\n",
    "        # Move to the next state\n",
    "        observation_tensor = next_observation_tensor\n",
    "\n",
    "        # Perform one step of the optimization (on the policy network)\n",
    "        optimize_policy_net()\n",
    "\n",
    "        # if done: \n",
    "        #     episode_durations.append(t + 1)\n",
    "        #     episode_rewards.append(total_reward)  # Save total reward for this episode\n",
    "        #     if truncated:\n",
    "        #         print(f\"Episode {i_episode + 1} truncated after {t + 1} steps\")\n",
    "        #     else:\n",
    "        #         print(f\"Episode {i_episode + 1} finished after {t + 1} steps\")\n",
    "        #     if i_episode % 10 == 0:\n",
    "        #         plot_training_durations()\n",
    "        #     break\n",
    "\n",
    "        if (t==99): \n",
    "            episode_durations.append(t + 1)\n",
    "            episode_rewards.append(total_reward)  # Save total reward for this episode\n",
    "\n",
    "            if i_episode % 10 == 0:\n",
    "                plot_training_durations()\n",
    "            \n",
    "        # Update target network (weights)\n",
    "        if t % TARGET_UPDATE == 0:\n",
    "            target_net.load_state_dict(policy_net.state_dict())\n",
    "    \n",
    "    print(i_episode + 1,'/',num_episodes, ', R= ', episode_rewards[i_episode])\n",
    "\n",
    "print('Training is finished')\n",
    "plt.ioff()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(policy_net.state_dict(), 'policy_net.pth')\n",
    "torch.save(target_net.state_dict(), 'target_net.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sam\n",
    "\n",
    "## test"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
