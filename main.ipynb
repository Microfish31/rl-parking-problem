{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Our custom module\n",
    "from nn import DeepQNetwork\n",
    "from custom_parking_env import ParkingWithObstacles\n",
    "from relay_mem import ReplayMemory,Transition\n",
    "\n",
    "# python module\n",
    "import gymnasium as gym\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.nn as nn\n",
    "from itertools import count\n",
    "from IPython import display\n",
    "    \n",
    "# interactive mode\n",
    "plt.ion()\n",
    "\n",
    "# Create the original environment and wrap it into an environment with obstacles\n",
    "env_origin = gym.make(\"parking-v0\")#, render_mode=\"human\")\n",
    "env = ParkingWithObstacles(env_origin)\n",
    "env.define_spaces()\n",
    "\n",
    "# terminated The episode is over if the ego vehicle crashed or the goal is reached or time is over.\n",
    "terminated = False\n",
    "\n",
    "# The episode is truncated if the time is over.\n",
    "truncated = False\n",
    "\n",
    "# Print the number of states and actions\n",
    "print(f\"Observation Space: {env.observation_space}\")\n",
    "# observation[\"observation\"] Box(-inf, inf, (6,), float64)\n",
    "# observation[\"achieved_goal\"] Box(-inf, inf, (6,), float64)\n",
    "# observation[\"desired_goal\"] Box(-inf, inf, (6,), float64)\n",
    "\n",
    "print(f\"Action Space: {env.action_space}\")\n",
    "# Discrete(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# combine observation\n",
    "def process_observation(observation):\n",
    "    observation_vector = np.concatenate((\n",
    "        observation[\"observation\"],\n",
    "        observation[\"achieved_goal\"],\n",
    "        observation[\"desired_goal\"]\n",
    "    ))\n",
    "    return observation_vector\n",
    "\n",
    "# if GPU is to be used\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "    print(\"Using CUDA\")\n",
    "elif torch.backends.mps.is_available():\n",
    "    device = torch.device(\"mps\")\n",
    "    print(\"Using MPS\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    print(\"Using CPU\")\n",
    "\n",
    "# BATCH_SIZE is the number of transitions sampled from the replay buffer\n",
    "# GAMMA is the discount factor as mentioned in the previous section\n",
    "# EPS_START is the starting value of epsilon\n",
    "# EPS_END is the final value of epsilon\n",
    "# EPS_DECAY controls the rate of exponential decay of epsilon, higher means a slower decay\n",
    "# TAU is the update rate of the target network\n",
    "# LR is the learning rate of the ``AdamW`` optimizer\n",
    "BATCH_SIZE = 2\n",
    "GAMMA = 0.99\n",
    "EPS_START = 0.9\n",
    "EPS_END = 0.05\n",
    "EPS_DECAY = 1000\n",
    "# TAU = 0.005\n",
    "TARGET_UPDATE = 50\n",
    "LR = 1e-4\n",
    "\n",
    "# Get number of actions from gym action space\n",
    "n_actions = env.action_space.n\n",
    "\n",
    "# Get the number of state observations\n",
    "state, info = env.reset()\n",
    "n_observations = len(process_observation(state))  # 6 (observation) + 6 (achieved_goal) + 6 (desired_goal)\n",
    "\n",
    "policy_net = DeepQNetwork(n_observations, n_actions).to(device)\n",
    "target_net = DeepQNetwork(n_observations, n_actions).to(device)\n",
    "target_net.load_state_dict(policy_net.state_dict())\n",
    "\n",
    "optimizer = optim.AdamW(policy_net.parameters(), lr=LR, amsgrad=True)\n",
    "memory = ReplayMemory(10000)\n",
    "\n",
    "steps_done = 0\n",
    "\n",
    "# Training loop\n",
    "num_episodes = 100\n",
    "epsilon_decay_rate = -np.log(EPS_END / EPS_START) / num_episodes\n",
    "epsilon_values = []\n",
    "episode_rewards = []  # Store total rewards per episode\n",
    "losses = []\n",
    "episode_durations = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_action(state,i_episode):\n",
    "    global steps_done\n",
    "    sample = random.random()\n",
    "    epsilon_threshold = EPS_START * np.exp(-epsilon_decay_rate * i_episode)\n",
    "    epsilon_values.append(epsilon_threshold)\n",
    "    steps_done += 1\n",
    "    if sample > epsilon_threshold:\n",
    "        with torch.no_grad():\n",
    "            # t.max(1) will return the largest column value of each row.\n",
    "            # second column on max result is index of where max element was\n",
    "            # found, so we pick action with the larger expected reward.\n",
    "            return policy_net(state).max(1).indices.view(1, 1)\n",
    "    else:\n",
    "        return torch.tensor([[env.action_space.sample()]], device=device, dtype=torch.long)\n",
    "\n",
    "def optimize_policy_net():\n",
    "    # Check if enough transitions are available in replay_buffer\n",
    "    if len(memory) < BATCH_SIZE:\n",
    "        return\n",
    "\n",
    "    # Sample a batch of transitions\n",
    "    transitions = memory.sample(BATCH_SIZE)\n",
    "    batch = Transition(*zip(*transitions))\n",
    "\n",
    "    # Create a mask for non-final next states\n",
    "    non_final_mask = torch.tensor(tuple(map(lambda s: s is not None, batch.next_state)), device=device, dtype=torch.bool)\n",
    "    non_final_next_states = torch.cat([s for s in batch.next_state if s is not None])\n",
    "    \n",
    "    # Concatenate batch tensors\n",
    "    state_batch = torch.cat(batch.state)\n",
    "    action_batch = torch.cat(batch.action)\n",
    "    reward_batch = torch.cat(batch.reward)\n",
    "\n",
    "    # Compute Q values for the current state-action pairs\n",
    "    state_action_values = policy_net(state_batch).gather(1, action_batch)\n",
    "\n",
    "    # Compute Q values for the next states\n",
    "    next_state_values = torch.zeros(BATCH_SIZE, device=device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        next_state_values[non_final_mask] = target_net(non_final_next_states).max(1).values\n",
    "    \n",
    "    # Compute the expected Q values\n",
    "    expected_state_action_values = (next_state_values * GAMMA) + reward_batch\n",
    "\n",
    "    # Compute Huber loss\n",
    "    criterion = nn.SmoothL1Loss()\n",
    "    loss = criterion(state_action_values, expected_state_action_values.unsqueeze(1))\n",
    "\n",
    "    # Optimize the policy network\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "\n",
    "    # In-place gradient clipping\n",
    "    # for param in policy_net.parameters():\n",
    "    #     param.grad.data.clamp_(-1, 1)\n",
    "    torch.nn.utils.clip_grad_value_(policy_net.parameters(), 100)\n",
    "    optimizer.step()\n",
    "\n",
    "    # Track the loss for plotting\n",
    "    losses.append(loss.item())\n",
    "    print(f\"Loss: {loss.item()}\")\n",
    "\n",
    "def moving_average(data, window_size=50):\n",
    "    # convolution\n",
    "    return np.convolve(data, np.ones(window_size)/window_size, mode='valid')\n",
    "\n",
    "def plot_training_durations():\n",
    "    # Create a new figure for the plot\n",
    "    plt.clf()\n",
    "\n",
    "    # Convert episode rewards to tensors\n",
    "    rewards_t = torch.tensor(episode_rewards, dtype=torch.float, device=device)\n",
    "\n",
    "    # Plot rewards per episode\n",
    "    plt.subplot(3, 1, 1)\n",
    "    plt.title('Training Progress (Per Episode)')\n",
    "    plt.xlabel('Episode')\n",
    "    plt.ylabel('Total Reward')\n",
    "    \n",
    "    # 將 GPU 張量移回 CPU 再轉換為 NumPy\n",
    "    plt.plot(rewards_t.cpu().numpy(), label=\"Total Reward\")\n",
    "    \n",
    "    # Add smoothed rewards\n",
    "    if len(episode_rewards) >= 50:  # Ensure enough data for smoothing\n",
    "        smoothed_rewards = moving_average(episode_rewards, window_size=50)\n",
    "        plt.plot(range(len(smoothed_rewards)), smoothed_rewards, label=\"Smoothed Rewards\", color='orange')\n",
    "    plt.legend()\n",
    "\n",
    "    # Plot losses per step\n",
    "    plt.subplot(3, 1, 2)\n",
    "    plt.title('Loss (Per Step)')\n",
    "    plt.xlabel('Training Step')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.plot(losses, label=\"Loss\", color='red')\n",
    "    plt.legend()\n",
    "\n",
    "    # Plot epsilon decay per step\n",
    "    plt.subplot(3, 1, 3)\n",
    "    plt.title('Epsilon Decay (Per Step)')\n",
    "    plt.xlabel('Training Step')\n",
    "    plt.ylabel('Epsilon')\n",
    "    plt.plot(epsilon_values, label=\"Epsilon Decay\", color='blue')\n",
    "    plt.legend()\n",
    "\n",
    "    # Adjust layout to increase space between plots\n",
    "    plt.subplots_adjust(hspace=0.5)  # Adjust spacing between rows\n",
    "\n",
    "    # Pause briefly to update the plot\n",
    "    plt.pause(0.01)\n",
    "\n",
    "    # Clear the current output and display the updated plot\n",
    "    display.display(plt.gcf())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i_episode in range(num_episodes):\n",
    "    # Initialize the environment and get its state\n",
    "    observation, info = env.reset()\n",
    "    observation_vector = process_observation(observation)\n",
    "    observation_tensor = torch.tensor(observation_vector, dtype=torch.float32, device=device).unsqueeze(0)\n",
    "    \n",
    "    total_reward = 0  # Track total reward for this episode\n",
    "\n",
    "    for t in count():\n",
    "        action = select_action(observation_tensor,i_episode)\n",
    "        observation, reward, done, truncated, _ = env.step(action.item())\n",
    "        reward = torch.tensor([reward], device=device)\n",
    "\n",
    "        total_reward += reward.item()  # Accumulate rewards\n",
    "\n",
    "        if done:\n",
    "            next_state = None\n",
    "        else:\n",
    "            observation_vector = process_observation(observation)\n",
    "            next_state = torch.tensor(observation_vector, dtype=torch.float32, device=device).unsqueeze(0)\n",
    "\n",
    "        # Store the transition in memory\n",
    "        memory.push(observation_tensor, action, next_state, reward)\n",
    "\n",
    "        # Move to the next state\n",
    "        observation_tensor = next_state\n",
    "\n",
    "        # Perform one step of the optimization (on the policy network)\n",
    "        optimize_policy_net()\n",
    "\n",
    "        if done:\n",
    "            episode_durations.append(t + 1)\n",
    "            episode_rewards.append(total_reward)  # Save total reward for this episode\n",
    "            if i_episode % 10 == 0:\n",
    "                plot_training_durations()\n",
    "            break\n",
    "\n",
    "        # Update target network\n",
    "        if t % TARGET_UPDATE == 0:\n",
    "            target_net.load_state_dict(policy_net.state_dict())\n",
    "    \n",
    "    print(i_episode,'/',num_episodes, ', R= ', episode_rewards[i_episode])\n",
    "\n",
    "print('Training is finished')\n",
    "plt.ioff()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Initialize the environment and get its state\n",
    "# observation, info = env.reset()\n",
    "# observation_vector = process_observation(observation)\n",
    "# observation_tensor = torch.tensor(observation_vector, dtype=torch.float32, device=device).unsqueeze(0)\n",
    "# print(observation_tensor.shape)\n",
    "# print(info)\n",
    "\n",
    "# action = select_action(observation_tensor,i_episode)\n",
    "# print(action)\n",
    "\n",
    "# observation, reward, done, truncated, _ = env.step(action.item())\n",
    "# print(observation)\n",
    "# print(done)\n",
    "\n",
    "# reward = torch.tensor([reward], device=device)\n",
    "# print(reward)\n",
    "# out = env.compute_reward(observation[\"achieved_goal\"],observation[\"desired_goal\"],{})\n",
    "# print(out > -env.config[\"success_goal_reward\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
