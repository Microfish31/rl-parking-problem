{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90d12e71-ba1b-4293-9b40-6f5c296bdca0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Our custom module\n",
    "from nn import DeepQNetwork\n",
    "from custom_parking_env import ParkingWithObstacles\n",
    "\n",
    "# python module\n",
    "import gymnasium as gym\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "# if GPU is to be used\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "    print(\"Using CUDA\")\n",
    "elif torch.backends.mps.is_available():\n",
    "    device = torch.device(\"mps\")\n",
    "    print(\"Using MPS\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    print(\"Using CPU\")\n",
    "    \n",
    "policy_net_weights = torch.load('policy_net.pth', weights_only=False)\n",
    "device = torch.device(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8931ec42-dc25-470a-8724-5cc88e5f3f7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "candidate_actions = []\n",
    "\n",
    "for steering in np.linspace(-0.5, 0.5, 11):\n",
    "        for acceleration in np.linspace(0.8, 0.4, 4):\n",
    "            candidate_actions.append(torch.Tensor([acceleration, steering]))\n",
    "            \n",
    "# print(candidate_actions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51c0f35f-d5cd-4c39-a397-5c5c64b10626",
   "metadata": {},
   "outputs": [],
   "source": [
    "# combine observation\n",
    "def process_observation(observation):\n",
    "    # observation_vector = np.concatenate((\n",
    "    #     observation[\"observation\"],\n",
    "    #     observation[\"achieved_goal\"],\n",
    "    #     observation[\"desired_goal\"]\n",
    "    # ))\n",
    "    return observation[\"observation\"]\n",
    "\n",
    "# Create the original environment and wrap it into an environment with obstacles\n",
    "env_origin = gym.make(\"parking-v0\", render_mode=\"human\")\n",
    "env = ParkingWithObstacles(env_origin)\n",
    "env.define_spaces()\n",
    "\n",
    "print(env.config)\n",
    "\n",
    "# Get the number of state observations\n",
    "state, info = env.reset()\n",
    "\n",
    "# Get number of actions from gym action space\n",
    "n_actions = len(candidate_actions)\n",
    "n_observations = len(process_observation(state))  # 6 (observation) \n",
    "\n",
    "print(env.observation_space)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16a2c838-f64f-4315-8d4c-5b4e6b76e6e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = DeepQNetwork(n_observations,n_actions).to(device)\n",
    "# Testing\n",
    "# Load the model weights\n",
    "model.load_state_dict(policy_net_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59313ff3-782c-4ee9-903d-0b98406c94c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(env, model, num_episodes=100):\n",
    "    model.eval()  # Set the model to evaluation mode\n",
    "    total_test_rewards = []\n",
    "    episode_durations = []\n",
    "    \n",
    "    def to_tensor(vector):\n",
    "        return torch.tensor(vector, dtype=torch.float32, device=device).unsqueeze(0)\n",
    "\n",
    "    for i_episode in range(num_episodes):\n",
    "        observation, info = env.reset()\n",
    "        observation_tensor = to_tensor(process_observation(observation))\n",
    "        \n",
    "        total_reward = 0\n",
    "        index = 0\n",
    "        print(f\"Demo Episode {i_episode + 1} started...\")\n",
    "\n",
    "        while True:\n",
    "            # print(env.time)\n",
    "            with torch.no_grad():\n",
    "                action_index = model(observation_tensor).max(1)[1].view(1, 1)\n",
    "\n",
    "            try:\n",
    "                action = candidate_actions[action_index.item()]\n",
    "            except IndexError:\n",
    "                print(f\"Invalid action index: {action_index.item()}\")\n",
    "                break\n",
    "\n",
    "            observation, reward, done, truncated, info = env.step(action.numpy())\n",
    "            total_reward += reward\n",
    "\n",
    "            if done or truncated:\n",
    "                print(f\"Done: {done}, Truncated: {truncated}\")\n",
    "                print(info)\n",
    "                print(f\"Episode {i_episode + 1} ended. Total Reward: {total_reward}\")\n",
    "                break\n",
    "\n",
    "            observation_tensor = to_tensor(process_observation(observation))\n",
    "            index += 1\n",
    "\n",
    "        episode_durations.append(index + 1)\n",
    "        total_test_rewards.append(total_reward)\n",
    "        print(f\"Progress: {i_episode + 1}/{num_episodes}\")\n",
    "\n",
    "    env.close()\n",
    "    avg_reward = np.mean(total_test_rewards)\n",
    "    avg_duration = np.mean(episode_durations)\n",
    "    print(f\"Test completed over {num_episodes} episodes.\")\n",
    "    print(f\"Average reward: {avg_reward:.2f}, Average duration: {avg_duration:.2f} steps.\")\n",
    "    print(f\"Max reward: {np.max(total_test_rewards):.2f}, Min reward: {np.min(total_test_rewards):.2f}\")\n",
    "\n",
    "\n",
    "test(env, model, num_episodes=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "917441fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "env.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
